{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd240cf5",
   "metadata": {},
   "source": [
    "<table style=\"width:100%; border-collapse: collapse;\">\n",
    "  <tr>\n",
    "    <td style=\"width:20%; vertical-align:middle;\">\n",
    "      <img src=\"LogoUVG.png\" width=\"400\"/>\n",
    "    </td>\n",
    "    <td style=\"text-align:left; vertical-align:middle;\">\n",
    "      <h2 style=\"margin-bottom: 0;\">Universidad del Valle de Guatemala - UVG</h2>\n",
    "      <h3 style=\"margin-top: 0;\">Facultad de Ingeniería - Computación</h3>\n",
    "      <p style=\"font-size: 16px; margin-bottom: 0; margin-top: -20px\">\n",
    "        <strong>Curso:</strong> CC3104 - Aprendizaje por Refuerzo \n",
    "        <strong>Sección:</strong> 10\n",
    "      </p>\n",
    "      <p style=\"font-size: 16px; margin: 0;\"><strong>Laboratorio 2:</strong> EDA</p>\n",
    "      <br>\n",
    "      <p style=\"font-size: 15px; margin: 0;\"><strong>Autores:</strong></p>\n",
    "      <ul style=\"margin-top: 5px; padding-left: 20px; font-size: 15px;\">\n",
    "        <li>Diego Alexander Hernández Silvestre - <strong>21270</strong></li>\n",
    "        <li>Linda Inés Jiménez Vides - <strong>21169</strong></li>\n",
    "        <li>Mario Antonio Guerra Morales - <strong>21008</strong></li>\n",
    "      </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb081d8",
   "metadata": {},
   "source": [
    "## 📝 Task 1\n",
    "Responda a cada de las siguientes preguntas de forma clara y lo más completamente posible.\n",
    "1. ¿Qué es un Markov Decision Process (MDP)?\n",
    "2. ¿Cuáles son los componentes principales de un MDP?\n",
    "3. ¿Cuál es el objetivo principal del aprendizaje por refuerzo con MDPs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4300126b",
   "metadata": {},
   "source": [
    "## 📝 Task 2\n",
    "\n",
    "1. Defina los componentes del MDP:\n",
    "    - Estados: S = {0, 1, 2, 3, 4, 5, 6, 7, 8}, donde cada número representa una celda del laberinto.\n",
    "    - Acciones: A = {arriba, abajo, izquierda, derecha}\n",
    "    - Probabilidades de transición: P(s' | s, a)\n",
    "    - Recompensas: R(s, a, s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea7eaaf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26e7f63a",
   "metadata": {},
   "source": [
    "2. Matriz de transición:\n",
    "    - Defina las probabilidades de transición P como un diccionario donde P[s][a] asigna los siguientes estados s' a sus probabilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb5325a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "706eda7a",
   "metadata": {},
   "source": [
    "3. Función de recompensa:\n",
    "    - Defina las recompensas R como un diccionario donde R[s][a][s'] da la recompensa por la transición del estado s al estado s' mediante la acción a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06918489",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94792fb5",
   "metadata": {},
   "source": [
    "4. Política:\n",
    "    - Defina una política π como un diccionario que asigna cada estado a una acción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f72d0f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28ff4b92",
   "metadata": {},
   "source": [
    "5. Simular la política:\n",
    "    - Escriba una función para simular la política en el MDP para una cierta cantidad de pasos.\n",
    "    - Realice un seguimiento de la recompensa acumulada obtenida siguiendo la política."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffa7d10",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f30809d",
   "metadata": {},
   "source": [
    "6. Evaluar la Política:\n",
    "    - Simule la póliza varias veces para estimar la recompensa acumulada promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c763439e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a7b8994",
   "metadata": {},
   "source": [
    "## 📝 Task 3\n",
    "En clase hemos dicho que una vez tengamos v* o q* sabemos la póliza óptima π* ¿Por qué?<br>\n",
    "Puede consultar el libro en la sección 3.8 en adelante"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
