{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd240cf5",
   "metadata": {},
   "source": [
    "<table style=\"width:100%; border-collapse: collapse;\">\n",
    "  <tr>\n",
    "    <td style=\"width:20%; vertical-align:middle;\">\n",
    "      <img src=\"LogoUVG.png\" width=\"400\"/>\n",
    "    </td>\n",
    "    <td style=\"text-align:left; vertical-align:middle;\">\n",
    "      <h2 style=\"margin-bottom: 0;\">Universidad del Valle de Guatemala - UVG</h2>\n",
    "      <h3 style=\"margin-top: 0;\">Facultad de Ingenier칤a - Computaci칩n</h3>\n",
    "      <p style=\"font-size: 16px; margin-bottom: 0; margin-top: -20px\">\n",
    "        <strong>Curso:</strong> CC3104 - Aprendizaje por Refuerzo \n",
    "        <strong>Secci칩n:</strong> 10\n",
    "      </p>\n",
    "      <p style=\"font-size: 16px; margin: 0;\"><strong>Laboratorio 2:</strong> EDA</p>\n",
    "      <br>\n",
    "      <p style=\"font-size: 15px; margin: 0;\"><strong>Autores:</strong></p>\n",
    "      <ul style=\"margin-top: 5px; padding-left: 20px; font-size: 15px;\">\n",
    "        <li>Diego Alexander Hern치ndez Silvestre - <strong>21270</strong></li>\n",
    "        <li>Linda In칠s Jim칠nez Vides - <strong>21169</strong></li>\n",
    "        <li>Mario Antonio Guerra Morales - <strong>21008</strong></li>\n",
    "      </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb081d8",
   "metadata": {},
   "source": [
    "## 游닇 Task 1\n",
    "Responda a cada de las siguientes preguntas de forma clara y lo m치s completamente posible.\n",
    "1. 쯈u칠 es un Markov Decision Process (MDP)?\n",
    "\n",
    "- Es un marco matem치tico con el cual se modelan decisiones en entornos estoc치sticos y secuenciales. Estos se basan en el principio de Markov, el cual establece que el estado futuro depende solamente del estado actual y la acci칩n que se toma, no todo el historial.\n",
    "\n",
    "2. 쮺u치les son los componentes principales de un MDP?\n",
    "\n",
    "- Sus componentes son: Estados (S), Acciones (A), Probabilidad de transici칩n (P), Recompensa (R), y el Factor de Descuento (풥).\n",
    "\n",
    "3. 쮺u치l es el objetivo principal del aprendizaje por refuerzo con MDPs?\n",
    "\n",
    "- Su objetivo principal es de aprender una p칩liza 칩ptima que maximice la recompensa obtenida y acumulada a largo plazo. La toma de decisiones que maximicen su beneficio total futuro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4300126b",
   "metadata": {},
   "source": [
    "## 游닇 Task 2\n",
    "\n",
    "1. Defina los componentes del MDP:\n",
    "    - Estados: S = {0, 1, 2, 3, 4, 5, 6, 7, 8}, donde cada n칰mero representa una celda del laberinto.\n",
    "    - Acciones: A = {arriba, abajo, izquierda, derecha}\n",
    "    - Probabilidades de transici칩n: P(s' | s, a)\n",
    "    - Recompensas: R(s, a, s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eea7eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estados y acciones\n",
    "states = list(range(9))  # 0 a 8\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "# Posiciones especiales\n",
    "start_state = 0\n",
    "goal_state = 8\n",
    "obstacle_states = [4]\n",
    "\n",
    "# Funci칩n auxiliar para obtener estado nuevo\n",
    "def move(state, action):\n",
    "    row, col = divmod(state, 3)\n",
    "    if action == 'up':\n",
    "        row = max(row - 1, 0)\n",
    "    elif action == 'down':\n",
    "        row = min(row + 1, 2)\n",
    "    elif action == 'left':\n",
    "        col = max(col - 1, 0)\n",
    "    elif action == 'right':\n",
    "        col = min(col + 1, 2)\n",
    "    new_state = 3 * row + col\n",
    "    # No permite moverse a obst치culos\n",
    "    if new_state in obstacle_states:\n",
    "        return state\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e7f63a",
   "metadata": {},
   "source": [
    "2. Matriz de transici칩n:\n",
    "    - Defina las probabilidades de transici칩n P como un diccionario donde P[s][a] asigna los siguientes estados s' a sus probabilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eb5325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Probabilidad de transici칩n determinista\n",
    "P = {s: {} for s in states}\n",
    "R = {s: {} for s in states}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706eda7a",
   "metadata": {},
   "source": [
    "3. Funci칩n de recompensa:\n",
    "    - Defina las recompensas R como un diccionario donde R[s][a][s'] da la recompensa por la transici칩n del estado s al estado s' mediante la acci칩n a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06918489",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in states:\n",
    "    for a in actions:\n",
    "        s_prime = move(s, a)\n",
    "        P[s][a] = {s_prime: 1.0}\n",
    "        \n",
    "        # Definir recompensas\n",
    "        if s_prime == goal_state:\n",
    "            reward = 10\n",
    "        elif s_prime in obstacle_states:\n",
    "            reward = -5\n",
    "        elif s == s_prime:\n",
    "            reward = -1  # penaliza quedarse en el mismo lugar\n",
    "        else:\n",
    "            reward = -0.1  # peque침a penalizaci칩n por moverse\n",
    "        R[s][a] = {s_prime: reward}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94792fb5",
   "metadata": {},
   "source": [
    "4. Pol칤tica:\n",
    "    - Defina una pol칤tica  como un diccionario que asigna cada estado a una acci칩n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f72d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28ff4b92",
   "metadata": {},
   "source": [
    "5. Simular la pol칤tica:\n",
    "    - Escriba una funci칩n para simular la pol칤tica en el MDP para una cierta cantidad de pasos.\n",
    "    - Realice un seguimiento de la recompensa acumulada obtenida siguiendo la pol칤tica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffa7d10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f30809d",
   "metadata": {},
   "source": [
    "6. Evaluar la Pol칤tica:\n",
    "    - Simule la p칩liza varias veces para estimar la recompensa acumulada promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c763439e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a7b8994",
   "metadata": {},
   "source": [
    "## 游닇 Task 3\n",
    "En clase hemos dicho que una vez tengamos v* o q* sabemos la p칩liza 칩ptima * 쯇or qu칠?<br>\n",
    "Puede consultar el libro en la secci칩n 3.8 en adelante"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
