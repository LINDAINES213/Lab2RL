{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd240cf5",
   "metadata": {},
   "source": [
    "<table style=\"width:100%; border-collapse: collapse;\">\n",
    "  <tr>\n",
    "    <td style=\"width:20%; vertical-align:middle;\">\n",
    "      <img src=\"LogoUVG.png\" width=\"400\"/>\n",
    "    </td>\n",
    "    <td style=\"text-align:left; vertical-align:middle;\">\n",
    "      <h2 style=\"margin-bottom: 0;\">Universidad del Valle de Guatemala - UVG</h2>\n",
    "      <h3 style=\"margin-top: 0;\">Facultad de Ingenier铆a - Computaci贸n</h3>\n",
    "      <p style=\"font-size: 16px; margin-bottom: 0; margin-top: -20px\">\n",
    "        <strong>Curso:</strong> CC3104 - Aprendizaje por Refuerzo \n",
    "        <strong>Secci贸n:</strong> 10\n",
    "      </p>\n",
    "      <p style=\"font-size: 16px; margin: 0;\"><strong>Laboratorio 2:</strong> EDA</p>\n",
    "      <br>\n",
    "      <p style=\"font-size: 15px; margin: 0;\"><strong>Autores:</strong></p>\n",
    "      <ul style=\"margin-top: 5px; padding-left: 20px; font-size: 15px;\">\n",
    "        <li>Diego Alexander Hern谩ndez Silvestre - <strong>21270</strong></li>\n",
    "        <li>Linda In茅s Jim茅nez Vides - <strong>21169</strong></li>\n",
    "        <li>Mario Antonio Guerra Morales - <strong>21008</strong></li>\n",
    "      </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb081d8",
   "metadata": {},
   "source": [
    "##  Task 1\n",
    "Responda a cada de las siguientes preguntas de forma clara y lo m谩s completamente posible.\n",
    "1. 驴Qu茅 es un Markov Decision Process (MDP)?\n",
    "2. 驴Cu谩les son los componentes principales de un MDP?\n",
    "3. 驴Cu谩l es el objetivo principal del aprendizaje por refuerzo con MDPs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4300126b",
   "metadata": {},
   "source": [
    "##  Task 2\n",
    "\n",
    "1. Defina los componentes del MDP:\n",
    "    - Estados: S = {0, 1, 2, 3, 4, 5, 6, 7, 8}, donde cada n煤mero representa una celda del laberinto.\n",
    "    - Acciones: A = {arriba, abajo, izquierda, derecha}\n",
    "    - Probabilidades de transici贸n: P(s' | s, a)\n",
    "    - Recompensas: R(s, a, s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea7eaaf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26e7f63a",
   "metadata": {},
   "source": [
    "2. Matriz de transici贸n:\n",
    "    - Defina las probabilidades de transici贸n P como un diccionario donde P[s][a] asigna los siguientes estados s' a sus probabilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb5325a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "706eda7a",
   "metadata": {},
   "source": [
    "3. Funci贸n de recompensa:\n",
    "    - Defina las recompensas R como un diccionario donde R[s][a][s'] da la recompensa por la transici贸n del estado s al estado s' mediante la acci贸n a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06918489",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94792fb5",
   "metadata": {},
   "source": [
    "4. Pol铆tica:\n",
    "    - Defina una pol铆tica  como un diccionario que asigna cada estado a una acci贸n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f72d0f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28ff4b92",
   "metadata": {},
   "source": [
    "5. Simular la pol铆tica:\n",
    "    - Escriba una funci贸n para simular la pol铆tica en el MDP para una cierta cantidad de pasos.\n",
    "    - Realice un seguimiento de la recompensa acumulada obtenida siguiendo la pol铆tica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffa7d10",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f30809d",
   "metadata": {},
   "source": [
    "6. Evaluar la Pol铆tica:\n",
    "    - Simule la p贸liza varias veces para estimar la recompensa acumulada promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c763439e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a7b8994",
   "metadata": {},
   "source": [
    "##  Task 3\n",
    "En clase hemos dicho que una vez tengamos v* o q* sabemos la p贸liza 贸ptima * 驴Por qu茅?<br>\n",
    "Puede consultar el libro en la secci贸n 3.8 en adelante"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
